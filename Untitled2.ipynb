{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1AszLqyyjSF"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openimages torch_snippets urllib3\n",
        "!wget -O open_images_train_captions.jsonl -q https://storage.googleapis.com/localized-narratives/annotations/open_images_train_v6_captions.jsonl\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field\n",
        "from pycocotools.coco import COCO\n",
        "from collections import defaultdict\n",
        "from torch_snippets import *\n",
        "import jsonimport numpy as np\n",
        "from openimages.download import _download_images_by_id\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "w3gB7OMuzniq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch the dataset from the Open Images dataset, which includes training images, their annotations, and the validation dataset and loop through the content of the JSON file and fetch the information of the first 100,000 images"
      ],
      "metadata": {
        "id": "Rkw2vDDA_Wgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('open_images_train_captions.jsonl', 'r') as json_file:\n",
        "    json_list = json_file.read().split('\\n')\n",
        "\n",
        "np.random.shuffle(json_list)\n",
        "\n",
        "data = []\n",
        "N = 100000\n",
        "\n",
        "for ix, json_str in tqdm(enumerate(json_list[:N])):\n",
        "    try:\n",
        "        result = json.loads(json_str)\n",
        "        x = pd.DataFrame.from_dict(result, orient='index').T\n",
        "        data.append(x)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "7jFWc7Qcy4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and validation datasets"
      ],
      "metadata": {
        "id": "tzt4G7qs_whp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "data = pd.concat(data)\n",
        "data['train'] = np.random.choice([True,False], \\\n",
        "                                 size=len(data),p=[0.90,0.10])\n",
        "data.to_csv('data.csv', index=False"
      ],
      "metadata": {
        "id": "0eenOjaty_9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the images corresponding to the image IDs fetched from the JSON file and save it in a new directory"
      ],
      "metadata": {
        "id": "55t0f0H7_43h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p train-images val-images\n",
        "subset_imageIds = data[data['train']].image_id.tolist()\n",
        "_download_images_by_id(subset_imageIds, 'train', \\\n",
        "                       './train-images/')\n",
        "\n",
        "subset_imageIds = data[~data['train']].image_id.tolist()\n",
        "_download_images_by_id(subset_imageIds, 'train', \\\n",
        "                       './val-images/')"
      ],
      "metadata": {
        "id": "jWgKL8I8zc7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a vocabulary of all the unique words present in all the captions in the dataframe"
      ],
      "metadata": {
        "id": "yA0TnQ0FADAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions = Field(sequential=False, init_token='<start>', \\\n",
        "                 eos_token='<end>')\n",
        "all_captions = data[data['train']]['caption'].tolist()\n",
        "all_tokens = [[w.lower() for w in c.split()] \\\n",
        "              for c in all_captions]\n",
        "all_tokens = [w for sublist in all_tokens \\\n",
        "              for w in sublist]\n",
        "captions.build_vocab(all_tokens)"
      ],
      "metadata": {
        "id": "JmT009J-zgU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a dummy vocab object, which is lightweight and will have an extra <pad> token "
      ],
      "metadata": {
        "id": "PnbHATEOASEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab: pass\n",
        "vocab = Vocab()\n",
        "captions.vocab.itos.insert(0, '<pad>')\n",
        "vocab.itos = captions.vocab.itos\n",
        "\n",
        "vocab.stoi = defaultdict(lambda: \\\n",
        "                         captions.vocab.itos.index('<unk>'))\n",
        "vocab.stoi['<pad>'] = 0\n",
        "for s,i in captions.vocab.stoi.items():\n",
        "    vocab.stoi[s] = i+1"
      ],
      "metadata": {
        "id": "Kghy5XUfz0NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the dataset class"
      ],
      "metadata": {
        "id": "hwBI9XIqAY3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptioningData(Dataset):\n",
        "    def __init__(self, root, df, vocab):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root = root\n",
        "        self.vocab = vocab\n",
        "        self.transform = transforms.Compose([ \n",
        "            transforms.Resize(224),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.RandomHorizontalFlip(), \n",
        "            transforms.ToTensor(), \n",
        "            transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                 (0.229, 0.224, 0.225))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns one pair of image and caption.\"\"\"\n",
        "        row = self.df.iloc[index].squeeze()\n",
        "        id = row.image_id\n",
        "        image_path = f'{self.root}/{id}.jpg'\n",
        "        image = Image.open(os.path.join(image_path))\\\n",
        "                                  .convert('RGB')\n",
        "\n",
        "        caption = row.caption\n",
        "        tokens = str(caption).lower().split()\n",
        "        target = []\n",
        "        target.append(vocab.stoi['<start>'])\n",
        "        target.extend([vocab.stoi[token] for token in tokens])\n",
        "        target.append(vocab.stoi['<end>'])\n",
        "        target = torch.Tensor(target).long()\n",
        "\n",
        "        return image, target, caption\n",
        "      \n",
        "    def choose(self):\n",
        "        return self[np.random.randint(len(self))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "  \n",
        "    def collate_fn(self, data):\n",
        "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, targets, captions = zip(*data)\n",
        "        images = torch.stack([self.transform(image) \\\n",
        "                              for image in images], 0)\n",
        "        lengths = [len(tar) for tar in targets]\n",
        "        _targets = torch.zeros(len(captions), \\\n",
        "                               max(lengths)).long()\n",
        "        for i, tar in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            _targets[i, :end] = tar[:end]\n",
        "             \n",
        "        return images.to(device), _targets.to(device), \\\n",
        "    torch.tensor(lengths).long().to(device)"
      ],
      "metadata": {
        "id": "fLEwI3hBz3Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training and validation dataset and data loaders"
      ],
      "metadata": {
        "id": "3P92bD_YAl34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ds = CaptioningData('train-images', data[data['train']], \\\n",
        "                        vocab)\n",
        "val_ds = CaptioningData('val-images', data[~data['train']], \\\n",
        "                        vocab)\n",
        "\n",
        "image, target, caption = trn_ds.choose()\n",
        "show(image, title=caption, sz=5); print(target)"
      ],
      "metadata": {
        "id": "k8U-by5N1F88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataloaders for the datasets"
      ],
      "metadata": {
        "id": "D-Jfxum3Aksi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_dl = DataLoader(trn_ds, 32, collate_fn=trn_ds.collate_fn)\n",
        "val_dl = DataLoader(val_ds, 32, collate_fn=val_ds.collate_fn)\n",
        "inspect(*next(iter(trn_dl)), names='images,targets,lengths')"
      ],
      "metadata": {
        "id": "e6vvsCz04VvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the encoder architecture â€“ use CNN and pre-trained resnet model"
      ],
      "metadata": {
        "id": "qh2O4eFUAx-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace \n",
        "        top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        # delete the last fc layer.\n",
        "        modules = list(resnet.children())[:-1] \n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, \\\n",
        "                                embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, \\\n",
        "                                 momentum=0.01)\n",
        "        \n",
        "  def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        \n",
        "        return features"
      ],
      "metadata": {
        "id": "hzyudp7M4lKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_summary"
      ],
      "metadata": {
        "id": "m9sS2wYV5unw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "AIDikdTD5vZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderCNN(256).to(device)\n",
        "\n",
        "print(summary(encoder,torch.zeros(32,3,224,224).to(device)))"
      ],
      "metadata": {
        "id": "KspAPDmF5jOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the decoder architecture â€“ RNN-LSTM model"
      ],
      "metadata": {
        "id": "mf3e5FzFBDpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, \\\n",
        "                 num_layers, max_seq_length=80):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, \\\n",
        "                            num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "  def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and \n",
        "        generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), \\\n",
        "                                embeddings), 1)\n",
        "        packed = pack_padded_sequence(embeddings, \\\n",
        "                            lengths.cpu(), batch_first=True) \n",
        "        outputs, _ = self.lstm(packed)\n",
        "        outputs = self.linear(outputs[0])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "  def predict(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image \n",
        "        features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states) \n",
        "            # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1)) \n",
        "            # outputs: (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1) \n",
        "            # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted) \n",
        "            # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1) \n",
        "            # inputs: (batch_size, 1, embed_size)\n",
        "\n",
        "        sampled_ids = torch.stack(sampled_ids, 1) \n",
        "        # sampled_ids: (batch_size, max_seq_length)\n",
        "        # convert predicted tokens to strings\n",
        "        sentences = []\n",
        "        for sampled_id in sampled_ids:\n",
        "            sampled_id = sampled_id.cpu().numpy()\n",
        "            sampled_caption = []\n",
        "            for word_id in sampled_id:\n",
        "                word = vocab.itos[word_id]\n",
        "                sampled_caption.append(word)\n",
        "                if word == '<end>':\n",
        "                    break\n",
        "            sentence = ' '.join(sampled_caption)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "  def train_batch(data, encoder, decoder, optimizer, criterion):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    images, captions, lengths = data\n",
        "    images = images.to(device)\n",
        "    captions = captions.to(device)\n",
        "    targets = pack_padded_sequence(captions, lengths.cpu(), \\\n",
        "                                   batch_first=True)[0]\n",
        "    features = encoder(images)\n",
        "    outputs = decoder(features, captions, lengths)\n",
        "    loss = criterion(outputs, targets)\n",
        "    decoder.zero_grad()\n",
        "    encoder.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def validate_batch(data, encoder, decoder, criterion):\n",
        "      encoder.eval()\n",
        "      decoder.eval()\n",
        "      images, captions, lengths = data\n",
        "      images = images.to(device)\n",
        "      captions = captions.to(device)\n",
        "      targets = pack_padded_sequence(captions, lengths.cpu(), \\\n",
        "                                    batch_first=True)[0]\n",
        "      features = encoder(images)\n",
        "      outputs = decoder(features, captions, lengths)\n",
        "      loss = criterion(outputs, targets)\n",
        "      \n",
        "      return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ye0Y_Iw5ywg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model objects and the loss function, and optimizer"
      ],
      "metadata": {
        "id": "yeYzS72xBo43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderCNN(256).to(device)\n",
        "decoder = DecoderRNN(256, 512, len(vocab.itos), 1).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(decoder.parameters()) + \\\n",
        "         list(encoder.linear.parameters()) + \\\n",
        "         list(encoder.bn.parameters())\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-3)\n",
        "n_epochs = 10\n",
        "log = Report(n_epochs)"
      ],
      "metadata": {
        "id": "Q45vxmNH9Cut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train the model over increasing epochs"
      ],
      "metadata": {
        "id": "P5p7UtuwBwgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    if epoch == 5: optimizer = torch.optim.AdamW(params, \\\n",
        "                                                 lr=1e-4)\n",
        "    N = len(trn_dl)\n",
        "    for i, data in enumerate(trn_dl):\n",
        "        trn_loss = train_batch(data, encoder, decoder, \\\n",
        "                               optimizer, criterion)\n",
        "        pos = epoch + (1+i)/N\n",
        "        log.record(pos=pos, trn_loss=trn_loss, end='\\r')\n",
        "\n",
        "    N = len(val_dl)\n",
        "    for i, data in enumerate(val_dl):\n",
        "        val_loss = validate_batch(data, encoder, decoder, \\\n",
        "                                  criterion)\n",
        "        pos = epoch + (1+i)/N\n",
        "        log.record(pos=pos, val_loss=val_loss, end='\\r')\n",
        "    log.report_avgs(epoch+1)\n",
        "\n",
        "\n",
        "log.plot_epochs(log=True)"
      ],
      "metadata": {
        "id": "q2Fk8Qoc9JyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize([224, 224], Image.LANCZOS)\n",
        "    if transform is not None:\n",
        "        tfm_image = transform(image)[None]\n",
        "\n",
        "    return image, tfm_image\n",
        "\n",
        "def load_image_and_predict(image_path):\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.ToTensor(), \n",
        "                    transforms.Normalize(\\\n",
        "                        (0.485, 0.456, 0.406), \n",
        "                        (0.229, 0.224, 0.225))\n",
        "                    ])\n",
        "    org_image, tfm_image = load_image(image_path, transform)\n",
        "    image_tensor = tfm_image.to(device)\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    feature = encoder(image_tensor)\n",
        "    sentence = decoder.predict(feature)[0]\n",
        "    show(org_image, title=sentence)\n",
        "\n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "5vPkw41L9yRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = Glob('val-images')\n",
        "load_image_and_predict(choose(files))"
      ],
      "metadata": {
        "id": "zRw6q6Fh-ScC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}